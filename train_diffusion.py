import os
import json
import time
import copy

from einops import repeat

import ml_collections
# è®¾ç½® matplotlib ä½¿ç”¨é GUI åç«¯ï¼Œé¿å… tkinter é”™è¯¯
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

import wandb

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split

import jax
import jax.numpy as jnp
from jax import random, jit, lax
from functools import partial

from jax.experimental import mesh_utils, multihost_utils
try:
    from jax.shard_map import shard_map
except ImportError:
    # å…¼å®¹æ—§ç‰ˆæœ¬ JAX
    from jax.experimental.shard_map import shard_map
from jax.sharding import Mesh, PartitionSpec as P

from function_diffusion.utils.data_utils import create_dataloader
from function_diffusion.utils.model_utils import (
    create_optimizer,
    create_autoencoder_state,
    create_diffusion_state,
    compute_total_params,
)
from function_diffusion.utils.train_utils import (
    create_train_diffusion_step,
    get_diffusion_batch,
    sample_ode,
    create_end_to_end_eval_step,
    create_autoencoder_eval_step,
)
from function_diffusion.utils.checkpoint_utils import (
    create_checkpoint_manager,
    save_checkpoint,
    restore_checkpoint,
    restore_fae_state
)

from model import DiT, Encoder, Decoder, DiffusionWrapper, ModelParamsAdapter
from model_utils import create_encoder_step, create_eval_step
from data_utils import generate_dataset
from geoelectric_dataset import log_normalize_data, log_denormalize_data, pad_or_trim


def _plot_inversion_result(y_true, y_pred, step, save_path, y_min=None, y_max=None, depth_range=(0, 1200), resistivity_range=(0, 200)):
    """
    ç»˜åˆ¶åæ¼”ç»“æœå¯è§†åŒ–ï¼šçœŸå®æ›²çº¿ vs é¢„æµ‹æ›²çº¿
    ä½¿ç”¨å’Œ MT1D_CNN_v1.py ç›¸åŒçš„ç»˜å›¾é£æ ¼
    """
    # y_true, y_pred shape: (batch, seq_len, 1)
    # å–ç¬¬ä¸€ä¸ªæ ·æœ¬æ¥ç”»
    y_true = np.array(y_true[0, :, 0]) if y_true.ndim == 3 else np.array(y_true[0, :])
    y_pred = np.array(y_pred[0, :, 0]) if y_pred.ndim == 3 else np.array(y_pred[0, :])
    
    # y ä¸å†å½’ä¸€åŒ–ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆä¸éœ€è¦åå½’ä¸€åŒ–ï¼‰
    
    # åˆ›å»ºæ·±åº¦åæ ‡
    num_points = len(y_true)
    depth_points = np.linspace(depth_range[0], depth_range[1], num_points)
    
    plt.figure(figsize=(10, 6))

    # ç»˜åˆ¶æ›²çº¿ï¼šä½¿ç”¨å’Œ MT1D_CNN_v1.py ç›¸åŒçš„æ ·å¼
    plt.plot(depth_points, y_true, linestyle='-', linewidth=2, color='blue', label='True $\\rho$ (Î©Â·m)')
    plt.plot(depth_points, y_pred, linestyle='--', linewidth=2, color='red', label='Predicted $\\rho$ (Î©Â·m)')

    # ä½¿ç”¨å¯¹æ•° Y è½´ï¼ˆå’Œ MT1D_CNN_v1.py ç›¸åŒï¼‰
    plt.yscale('log')
    
    # åŠ¨æ€è®¾ç½®Yè½´èŒƒå›´ï¼ˆå’Œ MT1D_CNN_v1.py ç›¸åŒï¼‰
    all_values = np.concatenate([y_true, y_pred])
    y_min_plot = max(0.01, np.min(all_values) * 0.8)  # ä½¿ç”¨0.01ä½œä¸ºæœ€å°è¾¹ç•Œï¼Œä¹˜ä»¥0.8ç•™å‡º20%è¾¹è·
    y_max_plot = min(10000, np.max(all_values) * 1.2)  # ä½¿ç”¨10000ä½œä¸ºæœ€å¤§è¾¹ç•Œï¼Œä¹˜ä»¥1.2ç•™å‡º20%è¾¹è·
    plt.ylim(y_min_plot, y_max_plot)

    # è®¾ç½®åæ ‡è½´èŒƒå›´å’Œæ ‡ç­¾
    plt.xlim(depth_range)
    
    # è®¾ç½®è‡ªå®šä¹‰åˆ»åº¦ï¼ˆå’Œ MT1D_CNN_v1.py ç›¸åŒï¼‰
    plt.xticks(np.arange(depth_range[0], depth_range[1] + 1, 400))
    
    # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
    plt.xlabel('Depth (m)', fontsize=10)
    plt.ylabel('Resistivity (Î©Â·m)', fontsize=10)
    
    # è®¡ç®—RMSEç”¨äºæ ‡é¢˜
    mse = np.mean((y_true - y_pred) ** 2)
    rmse = np.sqrt(mse)
    plt.title(f'Inversion Result (Step {step})\nValidation RMSE: {rmse:.4f}', fontsize=12)
    
    plt.grid(True, which='both', alpha=0.5)
    plt.legend(fontsize=10)

    # ä¿å­˜æ–‡ä»¶ï¼ˆå’Œ MT1D_CNN_v1.py ç›¸åŒï¼‰
    img_path = os.path.join(save_path, f"inversion_step_{step}.png")
    plt.savefig(img_path, dpi=200, bbox_inches='tight')
    plt.close()

    print(f"Saved inversion plot: {img_path}")


def plot_loss_curve(steps, train_losses, test_losses=None, test_steps=None, save_path=None):
    """
    ç»˜åˆ¶lossæ›²çº¿å¹¶ä¿å­˜
    Args:
        steps: è®­ç»ƒæ­¥æ•°åˆ—è¡¨
        train_losses: è®­ç»ƒlossåˆ—è¡¨
        test_losses: æµ‹è¯•lossåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        test_steps: æµ‹è¯•losså¯¹åº”çš„æ­¥æ•°åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        save_path: ä¿å­˜è·¯å¾„
    """
    plt.figure(figsize=(10, 6))
    plt.plot(steps, train_losses, 'b-', linewidth=2, label='Train Loss', alpha=0.7)
    
    if test_losses is not None and test_steps is not None:
        plt.plot(test_steps, test_losses, 'r-', linewidth=2, label='Test Loss', alpha=0.7)
    
    plt.xlabel('Training Step', fontsize=12)
    plt.ylabel('Loss', fontsize=12)
    plt.title('Training and Test Loss Curve', fontsize=14)
    plt.grid(True, alpha=0.3)
    plt.legend(fontsize=10)
    plt.yscale('log')  # ä½¿ç”¨å¯¹æ•°åˆ»åº¦ï¼Œå› ä¸ºlossé€šå¸¸å˜åŒ–å¾ˆå¤§
    
    # ä¿å­˜å›¾åƒ
    if save_path is None:
        save_path = os.path.join(os.getcwd(), "evaluation_plots")
    if not os.path.exists(save_path):
        os.makedirs(save_path)
    loss_plot_path = os.path.join(save_path, "training_loss.png")
    plt.savefig(loss_plot_path, dpi=200, bbox_inches='tight')
    plt.close()
    
    print(f"Lossæ›²çº¿å·²æ›´æ–°: {loss_plot_path}")


def plot_inversion_result(y_true, y_pred, step, save_path, y_min=None, y_max=None, depth_range=(0, 1200), resistivity_range=(0, 200)):
    """
    ç»˜åˆ¶åæ¼”ç»“æœå¯è§†åŒ–ï¼šçœŸå®æ›²çº¿ vs é¢„æµ‹æ›²çº¿
    ä½¿ç”¨å’Œ MT1D_CNN_v1.py ç›¸åŒçš„ç»˜å›¾é£æ ¼
    """
    # æ•°æ®éªŒè¯å’Œé¢„å¤„ç†
    print(f'y_true shape: {y_true.shape}, y_pred shape: {y_pred.shape}')
    try:
        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        y_true = np.array(y_true)
        y_pred = np.array(y_pred)
        
        # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
        if np.any(np.isnan(y_true)) or np.any(np.isnan(y_pred)):
            print(f"è­¦å‘Šï¼šæ­¥éª¤ {step} çš„æ•°æ®åŒ…å«NaNå€¼ï¼Œè·³è¿‡ç»˜å›¾")
            return
            
        if np.any(np.isinf(y_true)) or np.any(np.isinf(y_pred)):
            print(f"è­¦å‘Šï¼šæ­¥éª¤ {step} çš„æ•°æ®åŒ…å«æ— ç©·å€¼ï¼Œè·³è¿‡ç»˜å›¾")
            return
        
        # å¤„ç†æ•°æ®å½¢çŠ¶ï¼šç¡®ä¿æ˜¯1Dæ•°ç»„
        if y_true.ndim == 3:
            y_true = y_true[0, :, 0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ªé€šé“
        elif y_true.ndim == 2:
            y_true = y_true[0, :]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
        elif y_true.ndim == 1:
            pass  # å·²ç»æ˜¯1D
        else:
            print(f"è­¦å‘Šï¼šy_trueçš„å½¢çŠ¶ {y_true.shape} ä¸æ”¯æŒï¼Œè·³è¿‡ç»˜å›¾")
            return
            
        if y_pred.ndim == 3:
            y_pred = y_pred[0, :, 0]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç¬¬ä¸€ä¸ªé€šé“
        elif y_pred.ndim == 2:
            y_pred = y_pred[0, :]  # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
        elif y_pred.ndim == 1:
            pass  # å·²ç»æ˜¯1D
        else:
            print(f"è­¦å‘Šï¼šy_predçš„å½¢çŠ¶ {y_pred.shape} ä¸æ”¯æŒï¼Œè·³è¿‡ç»˜å›¾")
            return
        
        # æ£€æŸ¥æ•°æ®é•¿åº¦æ˜¯å¦ä¸€è‡´
        if len(y_true) != len(y_pred):
            print(f"è­¦å‘Šï¼šy_trueé•¿åº¦({len(y_true)})ä¸y_predé•¿åº¦({len(y_pred)})ä¸ä¸€è‡´")
            min_len = min(len(y_true), len(y_pred))
            y_true = y_true[:min_len]
            y_pred = y_pred[:min_len]
        
        # y ä¸å†å½’ä¸€åŒ–ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆä¸éœ€è¦åå½’ä¸€åŒ–ï¼‰
        
        # æ£€æŸ¥åå½’ä¸€åŒ–åçš„æ•°æ®èŒƒå›´
        y_true_range = (np.min(y_true), np.max(y_true))
        y_pred_range = (np.min(y_pred), np.max(y_pred))
        print(f"æ­¥éª¤ {step} - çœŸå®å€¼èŒƒå›´: {y_true_range}, é¢„æµ‹å€¼èŒƒå›´: {y_pred_range}")
        
        # ç¡®ä¿æ•°æ®éƒ½æ˜¯æ­£æ•°ï¼ˆå¯¹æ•°åˆ»åº¦è¦æ±‚ï¼‰
        y_true = np.maximum(y_true, 1e-6)
        y_pred = np.maximum(y_pred, 1e-6)
        
        # åˆ›å»ºæ·±åº¦åæ ‡
        num_points = len(y_true)
        depth_points = np.linspace(depth_range[0], depth_range[1], num_points)
        
        plt.figure(figsize=(10, 6))

        # ç»˜åˆ¶æ›²çº¿ï¼šä½¿ç”¨å’Œ MT1D_CNN_v1.py ç›¸åŒçš„æ ·å¼
        plt.plot(depth_points, y_true, linestyle='-', linewidth=2, color='blue', label='True $\\rho$ (Î©Â·m)')
        plt.plot(depth_points, y_pred, linestyle='--', linewidth=2, color='red', label='Predicted $\\rho$ (Î©Â·m)')

        # ä½¿ç”¨å¯¹æ•° Y è½´ï¼ˆå’Œ MT1D_CNN_v1.py ç›¸åŒï¼‰
        plt.yscale('log')
        
        # åŠ¨æ€è®¾ç½®Yè½´èŒƒå›´ï¼ˆå’Œ MT1D_CNN_v1.py ç›¸åŒï¼‰
        all_values = np.concatenate([y_true, y_pred])
        y_min_plot = max(0.01, np.min(all_values) * 0.8)  # ä½¿ç”¨0.01ä½œä¸ºæœ€å°è¾¹ç•Œï¼Œä¹˜ä»¥0.8ç•™å‡º20%è¾¹è·
        y_max_plot = min(10000, np.max(all_values) * 1.2)  # ä½¿ç”¨10000ä½œä¸ºæœ€å¤§è¾¹ç•Œï¼Œä¹˜ä»¥1.2ç•™å‡º20%è¾¹è·
        plt.ylim(y_min_plot, y_max_plot)

        # è®¾ç½®åæ ‡è½´èŒƒå›´å’Œæ ‡ç­¾
        plt.xlim(depth_range)
        
        # è®¾ç½®è‡ªå®šä¹‰åˆ»åº¦ï¼ˆå’Œ MT1D_CNN_v1.py ç›¸åŒï¼‰
        plt.xticks(np.arange(depth_range[0], depth_range[1] + 1, 400))
        
        # è®¾ç½®æ ‡ç­¾å’Œæ ‡é¢˜
        plt.xlabel('Depth (m)', fontsize=10)
        plt.ylabel('Resistivity (Î©Â·m)', fontsize=10)
        
        # è®¡ç®—RMSEç”¨äºæ ‡é¢˜
        mse = np.mean((y_true - y_pred) ** 2)
        rmse = np.sqrt(mse)
        plt.title(f'Inversion Result (Step {step})\nValidation RMSE: {rmse:.4f}', fontsize=12)
        
        plt.grid(True, which='both', alpha=0.5)
        plt.legend(fontsize=10)

        # ä¿å­˜æ–‡ä»¶ï¼ˆå’Œ MT1D_CNN_v1.py ç›¸åŒï¼‰
        img_path = os.path.join(save_path, f"inversion_step_{step}.png")
        plt.savefig(img_path, dpi=200, bbox_inches='tight')
        plt.close()

        print(f"Saved inversion plot: {img_path}")
            
    except Exception as e:
        print(f"ç»˜å›¾è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def train_and_evaluate(config: ml_collections.ConfigDict):
    # -------------------
    # 0) Initialize wandb (only on main process)
    # -------------------
    if jax.process_index() == 0:
        wandb.init(
            project=config.wandb.project,
            tags=[config.wandb.tag] if hasattr(config.wandb, 'tag') else [],
            config=config.to_dict(),
            name=f"{config.diffusion.model_name}_{config.dataset.num_samples}_samples"
        )
    
# -------------------
    # 1) Initialize autoencoder (load checkpoint if available)
    # -------------------
    encoder = Encoder(**config.autoencoder.encoder)
    decoder = Decoder(**config.autoencoder.decoder)
    
    # æ„é€ æ£€æŸ¥ç‚¹è·¯å¾„
    fae_job = f"{config.autoencoder.model_name}_{config.dataset.num_samples}_samples"
    fae_ckpt_path = os.path.join(os.getcwd(), fae_job, "ckpt")

    print("\n" + "="*50)
    print("åŠ è½½è‡ªç¼–ç å™¨æ¨¡å‹ (å¼ºåˆ¶å•é€šé“æ¨¡å¼)")
    print("="*50)
    print(f"æ£€æŸ¥ç‚¹è·¯å¾„: {fae_ckpt_path}")

    dummy_y = jnp.zeros((1, 50, 1)) 
    
    # è®¡ç®— Latent å½¢çŠ¶ (ç”¨äºè§£ç å™¨åˆå§‹åŒ–)
    patch_size = config.autoencoder.encoder.patch_size
    # å…¼å®¹ int æˆ– tuple æ ¼å¼
    p_size = patch_size[0] if isinstance(patch_size, tuple) else patch_size
    latent_len = 50 // p_size
    dummy_latent = jnp.zeros((1, latent_len, config.autoencoder.encoder.emb_dim))

    print("æ­£åœ¨åˆå§‹åŒ– FAE å‚æ•°ç»“æ„ (Target: 1 channel)...")
    rng = random.PRNGKey(0)
    rng, key_enc, key_dec = random.split(rng, 3)
    
    enc_variables = encoder.init(key_enc, dummy_y)
    dec_variables = decoder.init(key_dec, dummy_latent, dummy_y)
    
    initial_params = (enc_variables['params'], dec_variables['params'])
    
    _, tx_fae = create_optimizer(config) 
    
    from flax.training import train_state
    # åˆ›å»º State ç©ºå£³
    fae_state = train_state.TrainState.create(
        apply_fn=decoder.apply,
        params=initial_params, # è¿™é‡Œæ”¾å…¥äº†æ­£ç¡®çš„ 1 é€šé“å‚æ•°ç©ºå£³
        tx=tx_fae
    )

    # åˆ›å»º Checkpoint Manager
    ckpt_mngr = create_checkpoint_manager(config.saving, fae_ckpt_path)
    
    step = ckpt_mngr.latest_step()

    if step is not None:
        raw_restored = ckpt_mngr.restore(step)
        
        if 'params' in raw_restored:
            params_dict = raw_restored['params']
        elif 'model' in raw_restored and 'params' in raw_restored['model']:
            params_dict = raw_restored['model']['params']
        elif 'state' in raw_restored and 'params' in raw_restored['state']:
            params_dict = raw_restored['state']['params']
        else:
            print(f"è­¦å‘Š: æ£€æŸ¥ç‚¹ç»“æ„æœªçŸ¥ï¼Œå°è¯•ç›´æ¥è¯»å– keys: {raw_restored.keys()}")
            params_dict = raw_restored.get('params', raw_restored)

        fae_state = fae_state.replace(
            params=params_dict,
            step=step
        )
        
        print(f"æˆåŠŸåŠ è½½è‡ªç¼–ç å™¨æ£€æŸ¥ç‚¹ï¼åŠ è½½æ­¥æ•°: {step}")
        
    else:
        err_msg = (
            f"è‡´å‘½é”™è¯¯ï¼šåœ¨ {fae_ckpt_path} æœªæ‰¾åˆ° FAE æ£€æŸ¥ç‚¹ï¼\n"
            "Diffusion æ¨¡å‹ä¾èµ–é¢„è®­ç»ƒçš„è‡ªç¼–ç å™¨ã€‚\n"
            "è¯·å…ˆè¿è¡Œ 'python train_autoencoder.py'ã€‚"
        )
        raise RuntimeError(err_msg)
    
    print("="*50 + "\n")

     # -------------------
    # 2) Initialize diffusion model (Wrapper: CondEncoder + DiT)
    # -------------------
    # æ¡ä»¶ç¼–ç å™¨é…ç½®
    use_conditioning = True
    print("\n" + "="*50)
    print("åˆå§‹åŒ–æ‰©æ•£æ¨¡å‹ (Diffusion Model)")
    print("="*50)

    # 1. é…ç½® CondEncoder
    cond_config = ml_collections.ConfigDict(config.autoencoder.encoder)
    with cond_config.unlocked():
        cond_config.grid_size = (64,)
        cond_config.patch_size = (8,)
        cond_config.emb_dim = config.diffusion.emb_dim
        if 'input_dim' in cond_config: del cond_config.input_dim
    cond_encoder = Encoder(**cond_config)

    # 2. é…ç½® DiT
    diffusion_config = dict(config.diffusion)
    dit_supported_params = ['grid_size', 'emb_dim', 'depth', 'num_heads', 'mlp_ratio', 'out_dim']
    filtered_config = {k: v for k, v in diffusion_config.items() if k in dit_supported_params}
    dit_core = DiT(model_name=config.diffusion.model_name, **filtered_config)

    # 3. ç»„è£… Wrapper
    raw_model = DiffusionWrapper(dit=dit_core, cond_encoder=cond_encoder)
    model = ModelParamsAdapter(raw_model)
    print("âœ… å·²å¯ç”¨ ModelParamsAdapter ä»¥ä¿®å¤å‚æ•°æ ¼å¼å…¼å®¹æ€§é—®é¢˜ã€‚")
    
    p_size = config.autoencoder.encoder.patch_size
    if isinstance(p_size, tuple): p_size = p_size[0]
    latent_len = 50 // p_size
    
    # âš ï¸ æ³¨æ„ï¼šDiT è¾“å…¥çš„æ˜¯ Latentï¼ŒCondEncoder è¾“å…¥çš„æ˜¯åŸå§‹è§‚æµ‹æ•°æ®
    dummy_x = jnp.zeros((1, latent_len, config.diffusion.emb_dim)) # Latent Z
    dummy_t = jnp.zeros((1,), dtype=jnp.int32)
    dummy_c = jnp.zeros((1, 64, 2))  # <--- åŸå§‹æ¡ä»¶è¾“å…¥ (å¿…é¡»æ˜¯ 2 é€šé“)

    # 5. âœ… æ ¸å¿ƒä¿®æ­£ï¼šå®Œæ•´åˆå§‹åŒ–
    print("æ­£åœ¨åˆå§‹åŒ–å‚æ•°...")
    rng = random.PRNGKey(config.seed)
    rng, key_diff = random.split(rng)
    
    # å…³é”®ç‚¹ï¼šå¿…é¡»ä¼ å…¥ c=dummy_cï¼Œå¦åˆ™ cond_encoder ä¸ä¼šåˆ›å»ºå‚æ•°ï¼
    variables = model.init(key_diff, dummy_x, dummy_t, c=dummy_c)
    
    # 6. åˆ›å»º TrainState
    lr, tx = create_optimizer(config)
    from flax.training import train_state
    
    state = train_state.TrainState.create(
        apply_fn=model.apply,
        params=variables['params'],
        tx=tx
    )

    # 7. ç®€å•çš„ Checkpoint åŠ è½½ (åªä¸ºæ–­ç‚¹ç»­è®­ï¼Œä¸åšå¤æ‚åˆå¹¶)
    job_name = f"{config.diffusion.model_name}_{config.dataset.num_samples}_samples"
    ckpt_path = os.path.join(os.getcwd(), job_name, "ckpt")
    ckpt_mngr = create_checkpoint_manager(config.saving, ckpt_path)
    
    step = ckpt_mngr.latest_step()
    
    if step is not None:
        print(f"å‘ç°æ£€æŸ¥ç‚¹ (Step {step})ï¼Œæ­£åœ¨æ¢å¤...")
        # æ—¢ç„¶æ˜¯ä»å¤´è®­ç»ƒï¼Œå¦‚æœçœŸçš„å‘ç°äº†æ£€æŸ¥ç‚¹ï¼Œé‚£å°±ç›´æ¥è¦†ç›–ï¼Œä¸ç”¨è€ƒè™‘æ–°æ—§ç»“æ„å…¼å®¹
        restored = ckpt_mngr.restore(step)
        
        # ç®€å•çš„è§£åŒ…é€»è¾‘
        if 'params' in restored:
            loaded_params = restored['params']
        elif 'model' in restored:
            loaded_params = restored['model'].get('params', restored['model'])
        elif 'state' in restored:
            loaded_params = restored['state'].get('params', restored['state'])
        else:
            loaded_params = restored
            
        state = state.replace(params=loaded_params, step=step)
        print("âœ… çŠ¶æ€æ¢å¤æˆåŠŸã€‚")
    else:
        print("æœªå‘ç°æ£€æŸ¥ç‚¹ï¼Œä»å¤´å¼€å§‹è®­ç»ƒ (Fresh Start)ã€‚")

    # æ‰“å°å‚æ•°ç¡®è®¤
    num_params = sum(x.size for x in jax.tree_util.tree_leaves(state.params))
    print(f"Model parameters: {num_params / 1e6:.2f} M")
    print("="*50 + "\n")

    # -------------------
    # 3) Device / sharding
    # -------------------
    num_local_devices = jax.local_device_count()
    num_devices = jax.device_count()
    print(f"Number of devices: {num_devices}, local: {num_local_devices}")

    mesh = Mesh(mesh_utils.create_device_mesh((jax.device_count(),)), "batch")
    
    if jax.device_count() > 1:
        print("å¤šè®¾å¤‡ç¯å¢ƒï¼šæ­£åœ¨æ‰§è¡Œ host_local_array_to_global_array...")
        state = multihost_utils.host_local_array_to_global_array(state, mesh, P())
        fae_state = multihost_utils.host_local_array_to_global_array(fae_state, mesh, P())
    else:
        print("å•è®¾å¤‡ç¯å¢ƒï¼šè·³è¿‡ host_local_array_to_global_array (ç›´æ¥ä½¿ç”¨ Host æ•°æ®)ã€‚")

    print("\nğŸ” [æœ€ç»ˆæ£€æŸ¥] å‡†å¤‡è¿›å…¥è®­ç»ƒå¾ªç¯ï¼Œæ£€æŸ¥ state.params...")
    if 'cond_encoder' not in state.params:
        print("âŒ è‡´å‘½é”™è¯¯ï¼šcond_encoder åœ¨åˆ†ç‰‡/å‡†å¤‡é˜¶æ®µä¸¢å¤±ï¼")
        # è¿™é‡Œå¯ä»¥å†å°è¯•ä¸€æ¬¡ç´§æ€¥ä¿®å¤ï¼Œæˆ–è€…ç›´æ¥æŠ¥é”™åœæ­¢
        raise RuntimeError("å‚æ•°ä¸¢å¤±ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒã€‚")
    else:
        print("âœ… [æœ€ç»ˆæ£€æŸ¥] cond_encoder ä¾ç„¶å­˜åœ¨ã€‚Ready to train!")
    print("="*50 + "\n")

    print("\nğŸ” æ­£åœ¨æ£€æŸ¥ state.params å®Œæ•´æ€§...")
    

    # train / encoder steps
    train_step = create_train_diffusion_step(model, mesh, use_conditioning=use_conditioning)
    encoder_step = create_encoder_step(encoder, mesh)
    
    # åˆ›å»ºæµ‹è¯•é›†lossè®¡ç®—å‡½æ•°ï¼ˆä½¿ç”¨JITç¼–è¯‘ä»¥æé«˜æ•ˆç‡ï¼‰
    @jax.jit
    @partial(
        shard_map,
        mesh=mesh,
        in_specs=(P(), P(), P("batch"), P()),
        out_specs=P(),
        check_rep=False
    )
    def compute_test_loss(fae_state, diffusion_state, test_batch, rng):
        # ç¼–ç æµ‹è¯•æ•°æ®
        encoder_params, _ = fae_state.params
        coords_test, x_test, y_test = test_batch

        if x_test.shape[-1] == 2:
            x_test = jnp.mean(x_test, axis=-1, keepdims=True)

        z_u_test = encoder.apply(encoder_params, x_test)

        # ç”Ÿæˆdiffusion batch
        diff_batch_test, _ = get_diffusion_batch(rng, z1=z_u_test, c=None, use_conditioning=use_conditioning)

        if len(diff_batch_test) == 4:
            x, t, c, y = diff_batch_test
            pred = model.apply(diffusion_state.params, x, t, c)
            
        elif len(diff_batch_test) == 3:
            x, t, y = diff_batch_test
            pred = model.apply(diffusion_state.params, x, t)
        else:
            raise ValueError(f"Unexpected batch length: {len(diff_batch_test)}")
        
        eps = 1e-8
        batch_size, seq_len, channels = y.shape
        real_data_length = 50
        mask = jnp.arange(seq_len) < real_data_length
        mask = mask.astype(jnp.float32)
        mask = mask[None, :, None]
        mask = jnp.broadcast_to(mask, (batch_size, seq_len, channels))
        
        valid_count = jnp.sum(mask) + eps
        squared_error = (y - pred) ** 2
        masked_squared_error = squared_error * mask
        test_loss = jnp.sum(masked_squared_error) / valid_count
        test_loss = lax.pmean(test_loss, "batch")
        return test_loss

    # åˆ›å»ºä¿å­˜è·¯å¾„ç”¨äºå­˜å‚¨è¯„ä¼°å›¾åƒ
    save_path = os.path.join(os.getcwd(), "evaluation_plots")
    if not os.path.exists(save_path):
        os.makedirs(save_path)

        # -------------------
    # 4) Dataset and coords - KEEP CONSISTENT WITH FAE TRAINING
    # -------------------
    
    # è¯»å–æ•°æ®ï¼ˆä¸ train_autoencoder.py ä¿æŒä¸€è‡´ï¼‰
    train_data = pd.read_json('./train_data.json')
    num_samples = min(10000, len(train_data))
    train_data_sampled = train_data.sample(n=num_samples, random_state=42).reset_index(drop=True)

    # æå–æ•°æ® - ä½¿ç”¨æ­£ç¡®çš„åˆ—å
    try:
        train_rho = np.array([np.array(train_data_sampled['rho'][i]) for i in range(len(train_data_sampled))])
        train_phase = np.array([np.array(train_data_sampled['phase'][i]) for i in range(len(train_data_sampled))])
        train_res = np.array([np.array(train_data_sampled['res'][i]) for i in range(len(train_data_sampled))])
    except KeyError as e:
        print(f"è‡´å‘½é”™è¯¯ï¼šè®­ç»ƒæ•°æ® JSON æ–‡ä»¶ä¸­ç¼ºå°‘åˆ—å {e}ã€‚è¯·æ£€æŸ¥ train_data.jsonã€‚")
        raise
    
    # ã€æ£€æŸ¥åŸå§‹æ•°æ®èŒƒå›´ã€‘
    print("--- åŸå§‹ç›®æ ‡ç”µé˜»ç‡ (Î©Â·m) ç»Ÿè®¡ ---")
    print(f"åŸå§‹ train_res æœ€å°å€¼: {np.min(train_res)}")
    print(f"åŸå§‹ train_res æœ€å¤§å€¼: {np.max(train_res)}")
    # ä¿®å¤ nan é—®é¢˜ï¼šæ£€æŸ¥å¹¶æ›¿æ¢å°äºç­‰äºé›¶çš„å€¼ï¼ˆLog10 å˜æ¢å‰å¿…é¡»ä¿è¯æ•°æ® > 0ï¼‰
    train_res[train_res <= 0] = 1e-6
    # train_res = np.log10(train_res)  # æ³¨é‡Šæ‰ï¼Œå› ä¸ºæ•°æ®å·²ç»æ˜¯log10å€¼
    print(f"DEBUG: ç›®æ ‡å˜é‡ train_res å·²ç»æ˜¯Log10å°ºåº¦ï¼Œç›´æ¥ä½¿ç”¨ï¼Œå½¢çŠ¶: {train_res.shape}")
    print(f"Log10å€¼èŒƒå›´: [{np.min(train_res):.4f}, {np.max(train_res):.4f}]")
    print(f"å¯¹åº”çš„åŸå§‹ç”µé˜»ç‡èŒƒå›´: [10^{np.min(train_res):.4f}={10**np.min(train_res):.2f} Î©Â·m, "
          f"10^{np.max(train_res):.4f}={10**np.max(train_res):.2f} Î©Â·m]")

    # æ ‡å‡†åŒ–è§†ç”µé˜»ç‡æ•°æ® (Z-scoreæ ‡å‡†åŒ–)
    rho_mean = np.mean(train_rho)
    rho_dev = np.std(train_rho)
    train_rho_N = (train_rho - rho_mean) / rho_dev

    # æ ‡å‡†åŒ–ç›¸ä½æ•°æ® (Z-scoreæ ‡å‡†åŒ–)
    phase_mean = np.mean(train_phase)
    phase_dev = np.std(train_phase)
    train_phase_N = (train_phase - phase_mean) / phase_dev

    # è°ƒæ•´æ•°æ®ç»´åº¦ï¼šå…ˆå°†æ•°æ®pad/trimåˆ°ç›®æ ‡é•¿åº¦ï¼Œç„¶åreshape
    input_size = config.dataset.num_sensors  # ä½¿ç”¨é…ç½®ä¸­çš„ä¼ æ„Ÿå™¨æ•°é‡ (64)
    output_size = 50  # è¾“å‡ºåºåˆ—é•¿åº¦ä¸º50ï¼ˆä¸ train_autoencoder.py ä¸€è‡´ï¼‰
    
    # ä½¿ç”¨ pad_or_trim å°†æ¯ä¸ªæ ·æœ¬è°ƒæ•´åˆ°ç›®æ ‡é•¿åº¦
    train_rho_N = pad_or_trim(train_rho_N, input_size)
    train_phase_N = pad_or_trim(train_phase_N, input_size)
    train_res = pad_or_trim(train_res, output_size)  # è¾“å‡ºè°ƒæ•´ä¸º50
    
    # è°ƒæ•´ç»´åº¦ä¸º (num_samples, input_size, 1)
    train_rho_N = train_rho_N.reshape(-1, input_size, 1)
    train_phase_N = train_phase_N.reshape(-1, input_size, 1)
    x_train_normalized = np.concatenate([train_rho_N, train_phase_N], axis=2)

    # ========== yä¸è¿›è¡Œå½’ä¸€åŒ–ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼ˆä¸ train_autoencoder.py ä¸€è‡´ï¼‰==========
    # è°ƒæ•´ y çš„ç»´åº¦ä¸º (num_samples, output_size, 1) = (num_samples, 50, 1)
    y_train = train_res.reshape(-1, output_size, 1)  # ç›´æ¥ä½¿ç”¨åŸå§‹æ•°æ®ï¼Œä¸å½’ä¸€åŒ–
    
    print(f"\n{'='*50}")
    print("æ‰©æ•£æ¨¡å‹è¾“å…¥è¾“å‡ºå½¢çŠ¶ä¿¡æ¯")
    print(f"{'='*50}")
    print(f"è¾“å…¥ x_train_normalized å½¢çŠ¶: {x_train_normalized.shape}")
    print(f"  - è¯´æ˜: (æ ·æœ¬æ•°, åºåˆ—é•¿åº¦={input_size}, é€šé“æ•°=2)")
    print(f"  - é€šé“: [rho(æ ‡å‡†åŒ–å), phase(æ ‡å‡†åŒ–å)]")
    print(f"è¾“å‡º y_train å½¢çŠ¶: {y_train.shape}")
    print(f"  - è¯´æ˜: (æ ·æœ¬æ•°, åºåˆ—é•¿åº¦={output_size}, é€šé“æ•°=1)")
    print(f"  - é€šé“: [res(åŸå§‹æ•°æ®, log10å°ºåº¦, æœªå½’ä¸€åŒ–)]")
    print(f"yæ•°æ®èŒƒå›´: [{y_train.min():.3f}, {y_train.max():.3f}]")
    print(f"{'='*50}\n")

    # åˆå¹¶è¾“å…¥ï¼ˆä¿æŒä¸è‡ªç¼–ç å™¨è®­ç»ƒä¸€è‡´ï¼‰
    condition_data = x_train_normalized  # åŒ…å«æ ‡å‡†åŒ–åçš„ rho å’Œ phase

    # IMPORTANT: coords must match the shape used when training the autoencoder.
    # ç”Ÿæˆè¾“å‡ºåæ ‡ï¼ˆ50ä¸ªç‚¹ï¼ŒåŒ¹é…è¾“å‡ºç»´åº¦ï¼Œä¸ train_autoencoder.py ä¸€è‡´ï¼‰
    coords = np.linspace(0, 1, output_size)[:, None]  # shape (output_size, 1) = (50, 1)

    # Repeat coords across devices: shape (n_devices, num_sensors, 1)
    batch_coords = repeat(coords, "b d -> n b d", n=jax.device_count())

    batch = (batch_coords, condition_data, y_train) 
    batch = jax.tree.map(jnp.array, batch)
    batch = multihost_utils.host_local_array_to_global_array(batch, mesh, P("batch"))

    # If checkpoint dir doesn't exist, create and save config
    if jax.process_index() == 0:
        if not os.path.isdir(ckpt_path):
            os.makedirs(ckpt_path)
        # save config
        config_dict = config.to_dict()
        with open(os.path.join(os.getcwd(), job_name, "config.json"), "w") as f:
            json.dump(config_dict, f, indent=4)

    # Ensure ckpt manager exists
    ckpt_mngr = create_checkpoint_manager(config.saving, ckpt_path)

    # -------------------
    # 5) Prepare test set (keep shapes consistent)
    # -------------------
    test_data_path = os.path.join(os.getcwd(), fae_job, "test_data.npz")
    print(f"Looking for test data at: {test_data_path}")
    if os.path.exists(test_data_path):
        print("âœ… åŠ è½½è‡ªç¼–ç å™¨çš„æµ‹è¯•é›†...")
        test_data = np.load(test_data_path)
        x_test = test_data['x_test']
        y_test = test_data['y_test']
    
        print(f"   æµ‹è¯•é›†å½¢çŠ¶: x_test{x_test.shape}, y_test{y_test.shape}")
        print(f"   y_test æ•°æ®èŒƒå›´: [{y_test.min():.3f}, {y_test.max():.3f}] (åŸå§‹æ•°æ®ï¼Œæœªå½’ä¸€åŒ–)")
    else:
        # å¦‚æœæ²¡æœ‰æµ‹è¯•æ•°æ®æ–‡ä»¶ï¼Œä»è®­ç»ƒæ•°æ®ä¸­åˆ†å‰²ä¸€éƒ¨åˆ†ä½œä¸ºæµ‹è¯•é›†
        print("âš ï¸ æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨ train_test_split ä»è®­ç»ƒæ•°æ®ä¸­åˆ†å‰²20%ä½œä¸ºæµ‹è¯•é›†...")
        
        # ä½¿ç”¨ train_test_split åˆ’åˆ†æ•°æ® (80/20 åˆ’åˆ†)
        x_train_normalized, x_test, y_train, y_test = train_test_split(
            x_train_normalized, y_train, test_size=0.2, random_state=42
        )
        
        # æ›´æ–°è®­ç»ƒæ•°æ®
        condition_data = x_train_normalized
        y_train = y_train
        
        print(f"   è®­ç»ƒé›†: x_train{x_train_normalized.shape}, y_train{y_train.shape}")
        print(f"   æµ‹è¯•é›†: x_test{x_test.shape}, y_test{y_test.shape}")

    # ä½¿ç”¨æµ‹è¯•æ•°æ®ï¼ˆx_test å’Œ y_test éƒ½æ˜¯åŸå§‹/æ ‡å‡†åŒ–åçš„æ•°æ®ï¼Œæœªå½’ä¸€åŒ–ï¼‰
    condition_data_test = x_test  # åŒ…å«æ ‡å‡†åŒ–åçš„ rho å’Œ phase

    batch_coords_test = repeat(coords, "b d -> n b d", n=jax.device_count())
    test_batch = (batch_coords_test, condition_data_test, y_test)
    test_batch = jax.tree.map(jnp.array, test_batch)
    test_batch = multihost_utils.host_local_array_to_global_array(test_batch, mesh, P("batch"))

    # -------------------
    # 6) End-to-end eval step & autoencoder eval step
    # -------------------
    end_to_end_eval_step = create_end_to_end_eval_step(encoder, decoder, model, mesh, use_conditioning=use_conditioning)
    # è‡ªç¼–ç å™¨è¯„ä¼°ä½¿ç”¨ä¸ evaluate_autoencoder.py ä¸€è‡´çš„ eval_stepï¼ˆè¿”å› MSEï¼‰
    autoencoder_mse_eval_step = create_eval_step(encoder, decoder, mesh)

    # -------------------
    # 7) Training loop
    # -------------------
    rng = random.PRNGKey(config.training.seed if 'seed' in config.training else 0)
    
    # åˆå§‹åŒ–lossåˆ—è¡¨ç”¨äºç»˜åˆ¶æ›²çº¿
    train_loss_history = []
    test_loss_history = []
    train_step_history = []
    test_step_history = []
    
    for step in range(config.training.max_steps):
        start_time = time.time()
        rng, _ = random.split(rng)

        batch_coords, x_obs, y_true = batch
        z_target = encoder_step(fae_state.params[0], (batch_coords, y_true, y_true))
        c_for_model = x_obs
    
        
        diff_batch, rng = get_diffusion_batch(
            rng,
            z1=z_target,          
            c=c_for_model,   
            use_conditioning=use_conditioning 
        )
        state, loss = train_step(state, diff_batch)

        # Logging
        if step % config.logging.log_interval == 0:
            loss_val = float(loss)
            end_time = time.time()
            if jax.process_index() == 0:
                print(f"step: {step}, loss: {loss_val:.3e}, time: {end_time - start_time:.3f}")
                # æ”¶é›†è®­ç»ƒlosså€¼ç”¨äºç»˜å›¾
                train_loss_history.append(loss_val)
                train_step_history.append(step)
                # æ›´æ–°losså›¾
                if len(train_loss_history) > 0:
                    plot_loss_curve(
                        train_step_history, 
                        train_loss_history, 
                        test_losses=test_loss_history if len(test_loss_history) > 0 else None,
                        test_steps=test_step_history if len(test_step_history) > 0 else None,
                        save_path=save_path
                    )
                # Log to wandb
                wandb.log({
                    "train_loss": loss_val,
                    "learning_rate": lr(step),
                    "step": step,
                    "time_per_step": end_time - start_time
                }, step=step)

        # Periodic end-to-end evaluation (ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è¯„ä¼°é—´éš”)
        if step % config.logging.eval_interval == 0 and step > 0:
            try:
                # ç°åœ¨è¿”å›ä¸¤ä¸ªå€¼ï¼šrmse, normalized_rmse
                rmse_val, normalized_rmse_val, y_pred_val, y_true_val = end_to_end_eval_step(
                    fae_state, state, test_batch
                )

                # print(f'pred_res = {y_pred_val}')
                # print(f'true_res = {y_true_val}')

                if jax.process_index() == 0:  # åªåœ¨ä¸»è¿›ç¨‹ç”»å›¾
                    plot_inversion_result(
                        y_true_val, y_pred_val,
                        step,
                        save_path
                        # y_min å’Œ y_max ä¸å†éœ€è¦ï¼Œå› ä¸º y ä¸å½’ä¸€åŒ–
                    )
                rmse_val = float(rmse_val) if rmse_val is not None else None
                normalized_rmse_val = float(normalized_rmse_val) if normalized_rmse_val is not None else None
            except Exception as e:
                rmse_val, normalized_rmse_val = None, None
                print("End-to-end eval failed:", e)
                import traceback
                traceback.print_exc()
            
            if jax.process_index() == 0:
                if rmse_val is not None and normalized_rmse_val is not None:
                    print(f"step: {step}, diffusion_loss: {float(loss):.3e}, end_to_end_rmse: {rmse_val:.3e}, normalized_rmse: {normalized_rmse_val:.3f}")
                    # Log evaluation metrics to wandb
                    wandb.log({
                        "eval/end_to_end_rmse": rmse_val,
                        "eval/normalized_rmse": normalized_rmse_val,
                        "eval/inversion_plot": wandb.Image(os.path.join(save_path, f"inversion_step_{step}.png"))
                    }, step=step)
                else:
                    print(f"step: {step}, diffusion_loss: {float(loss):.3e}, end_to_end_rmse: N/A")

        # Save checkpoint at intervals
        if step % config.saving.save_interval == 0:
            if jax.process_index() == 0:
                loss_val = float(loss)
                save_checkpoint(ckpt_mngr, state)
                # æ„å»ºæ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ï¼ˆorbaxé€šå¸¸ä»¥stepå‘½åç›®å½•ï¼‰
                ckpt_file_path = os.path.join(ckpt_path, str(step))
                print(f"ğŸ’¾ Saving checkpoint at step {step}, diffusion_loss: {loss_val:.3e}")
                print(f"   æ£€æŸ¥ç‚¹æ–‡ä»¶: {ckpt_file_path}")
            else:
                save_checkpoint(ckpt_mngr, state)

    # Save final checkpoint
    print("\n" + "="*50)
    print("è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    print("="*50)
    
    if jax.process_index() == 0:
        print(f"æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜è·¯å¾„: {ckpt_path}")
    
    save_checkpoint(ckpt_mngr, state)
    ckpt_mngr.wait_until_finished()
    
    if jax.process_index() == 0:
        # æ„å»ºæœ€ç»ˆæ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨state.stepè·å–å½“å‰æ­¥æ•°ï¼‰
        final_step = int(state.step) if hasattr(state, 'step') else (config.training.max_steps - 1)
        final_ckpt_file = os.path.join(ckpt_path, str(final_step))
        print("âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜å®Œæˆï¼")
        print(f"   æ£€æŸ¥ç‚¹ç›®å½•: {ckpt_path}")
        print(f"   æ£€æŸ¥ç‚¹æ–‡ä»¶: {final_ckpt_file}")
        print(f"   å¯é€šè¿‡ restore_checkpoint å‡½æ•°åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†")
        print("="*50)
    
    # è®¡ç®—æœ€ç»ˆæµ‹è¯•é›†loss
    print("\n" + "="*50)
    print("è®¡ç®—æœ€ç»ˆæµ‹è¯•é›†loss...")
    print("="*50)
    try:
        rng_test, _ = random.split(rng)
        test_loss = compute_test_loss(fae_state, state, test_batch, rng_test)
        test_loss_val = float(test_loss)
        
        if jax.process_index() == 0:
            # æ”¶é›†æœ€ç»ˆæµ‹è¯•losså€¼ç”¨äºç»˜å›¾
            test_loss_history.append(test_loss_val)
            test_step_history.append(final_step)
            print(f"âœ… æœ€ç»ˆæµ‹è¯•é›†loss: {test_loss_val:.3e}")
            # æ›´æ–°åŒ…å«æµ‹è¯•lossçš„losså›¾
            if len(test_loss_history) > 0:
                plot_loss_curve(
                    train_step_history, 
                    train_loss_history, 
                    test_losses=test_loss_history,
                    test_steps=test_step_history,
                    save_path=save_path
                )
                # Log final test loss to wandb
                wandb.log({
                    "test_loss": test_loss_val,
                    "training_loss_curve": wandb.Image(os.path.join(save_path, "training_loss.png"))
                }, step=final_step)
    except Exception as e:
        test_loss_val = None
        if jax.process_index() == 0:
            print(f"âš ï¸ è®¡ç®—æµ‹è¯•é›†losså¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    print("="*50)

    # -------------------
    # 8) Unified evaluation (use test set, shapes kept consistent)
    # -------------------
    print("\n" + "="*50)
    print("å¼€å§‹ç»Ÿä¸€æ¨¡å‹è¯„ä¼°")
    print("="*50)

    print("1. è¯„ä¼°è‡ªç¼–ç å™¨é‡å»ºæ€§èƒ½...")
    try:
        # ä¸ evaluate_autoencoder.py ä¸€è‡´ï¼šå…ˆè®¡ç®— MSEï¼Œå†è½¬ RMSE / NRMSE
        ae_mse = autoencoder_mse_eval_step(fae_state, test_batch)
        ae_mse = float(ae_mse)
        autoencoder_rmse = np.sqrt(ae_mse)
        autoencoder_normalized_rmse = autoencoder_rmse / 2.106
    except Exception as e:
        autoencoder_rmse, autoencoder_normalized_rmse = None, None
        print("Autoencoder eval failed:", e)

    print("2. è¯„ä¼°æ‰©æ•£æ¨¡å‹ç”Ÿæˆæ€§èƒ½...")
    try:
        # ç°åœ¨è¿”å›å››ä¸ªå€¼ï¼šrmse, normalized_rmse
        diffusion_rmse, diffusion_normalized_rmse, _, _ = end_to_end_eval_step(fae_state, state, test_batch)
        diffusion_rmse = float(diffusion_rmse) if diffusion_rmse is not None else None
        diffusion_normalized_rmse = float(diffusion_normalized_rmse) if diffusion_normalized_rmse is not None else None
    except Exception as e:
        diffusion_rmse, diffusion_normalized_rmse = None, None
        print("End-to-end diffusion eval failed:", e)

    print("\n" + "="*50)
    print("æ¨¡å‹è¯„ä¼°ç»“æœ")
    print("="*50)

    if jax.process_index() == 0:
        if autoencoder_rmse is not None and autoencoder_normalized_rmse is not None:
            print(f"è‡ªç¼–ç å™¨ - RMSE: {autoencoder_rmse:.6f}, NRMSE: {autoencoder_normalized_rmse:.6f} ({autoencoder_normalized_rmse*100:.1f}%)")
        else:
            print("è‡ªç¼–ç å™¨æµ‹è¯•: è¯„ä¼°å¤±è´¥")
        
        if diffusion_rmse is not None and diffusion_normalized_rmse is not None:
            print(f"æ‰©æ•£æ¨¡å‹ç«¯åˆ°ç«¯ - RMSE: {diffusion_rmse:.6f}, NRMSE: {diffusion_normalized_rmse:.6f} ({diffusion_normalized_rmse*100:.1f}%)")
        else:
            print("æ‰©æ•£æ¨¡å‹ç«¯åˆ°ç«¯: è¯„ä¼°å¤±è´¥")

        if autoencoder_normalized_rmse is not None and diffusion_normalized_rmse is not None:
            print(f"æ€§èƒ½å¯¹æ¯”: æ‰©æ•£æ¨¡å‹æ¯”è‡ªç¼–ç å™¨ {'æ›´å¥½' if diffusion_normalized_rmse < autoencoder_normalized_rmse else 'ç¨å·®'}")
            # Log final evaluation metrics to wandb
            wandb.log({
                "final/autoencoder_rmse": autoencoder_rmse,
                "final/autoencoder_normalized_rmse": autoencoder_normalized_rmse,
                "final/diffusion_rmse": diffusion_rmse,
                "final/diffusion_normalized_rmse": diffusion_normalized_rmse,
                "final/diffusion_better": diffusion_normalized_rmse < autoencoder_normalized_rmse
            })

    # Finish wandb run
    if jax.process_index() == 0:
        wandb.finish()

    print("æ‰€æœ‰æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼")
    print("="*50)

   
